{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0QtxQ2slUx_i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Run first"
      ],
      "metadata": {
        "id": "w_fRFl5sUnvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "o4ZrazPrUjqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oNClFCrjUk8K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "0QtxQ2slUx_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start VADER\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "word = 'search find'\n",
        "vs = analyzer.polarity_scores(word)\n",
        "print(\"{:-<65} {}\".format(word, str(vs)))"
      ],
      "metadata": {
        "id": "VxKCqjxSUmjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts text to a SA vector through VADER\n",
        "\n",
        "def text_2_sent_vec(text):\n",
        "  vs = analyzer.polarity_scores(text)\n",
        "  sent_vec = np.zeros((len(vs)))\n",
        "  for idx, key in enumerate(vs):\n",
        "    sent_vec[idx] = vs[key]\n",
        "\n",
        "  return sent_vec"
      ],
      "metadata": {
        "id": "TmwAGfLdU8fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_2_sent_vec(\"funeral somber\")"
      ],
      "metadata": {
        "id": "QZSu3RW5U9rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Ycb-c_UBoj"
      },
      "outputs": [],
      "source": [
        "# Testing analogy pairs with VADER\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "questions = [[\"diamond:baseball\", \"court:poker\", \"court:jury\", \"court:grass\", \"court:squash\", 4],\n",
        "             [\"bench:judge\", \"throne:king\", \"queen:king\", \"court:king\", \"knight:king\", 3],\n",
        "             [\"funeral:somber\", \"tension:festive\", \"soiree:festive\", \"eulogy:festive\", \"sari:festive\", 2],\n",
        "             [\"defeat:vanquish\", \"search:peer\", \"search:ransack\", \"search:destroy\", \"search:find\", 4],\n",
        "             [\"slug:land\", \"shark:seaweed\", \"shark:ocean\", \"shark:sky\", \"shark:slide\", 2]]\n",
        "\n",
        "analogy_predictions = []\n",
        "\n",
        "for p in range(0, len(questions)):\n",
        "  print(\"Q\"+ str(p+1) +\":\")\n",
        "  q = questions[p]\n",
        "  \n",
        "  a = q[0].split(':')\n",
        "  a_combined = a[0] + ' ' + a[1]\n",
        "  a_vec = text_2_sent_vec(a_combined)\n",
        "  print(a_vec)\n",
        "\n",
        "  sim_scores = []\n",
        "  for i in range(1,5):\n",
        "    b = q[i].split(':')\n",
        "    b_combined = b[0] + ' ' + b[1]\n",
        "    b_vec = text_2_sent_vec(b_combined)\n",
        "    print(b_vec)\n",
        "\n",
        "    sim = 1 - spatial.distance.cosine(a_vec, b_vec)\n",
        "    sim_scores.append(sim)\n",
        "\n",
        "  print(\"Similarities:\")\n",
        "  print(sim_scores)\n",
        "  analogy_predictions.append(np.argmax(sim_scores)+1)\n",
        "  print()\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "correct_anal = 0\n",
        "for i in range (0,len(questions)):\n",
        "  if questions[i][5] == analogy_predictions[i]:\n",
        "    print(\"Correct answer for Question #\" + str(i+1))\n",
        "  else:\n",
        "    print(\"Incorrect answer for Question #\" + str(i+1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with synonyms/antonyms\n",
        "\n",
        "# Need CSV from Project 3\n",
        "syntest = pd.read_csv(\"syntest.csv\")\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "prediction_list = []\n",
        "\n",
        "for k in range(0,len(syntest)):\n",
        "  t = syntest.loc[k]\n",
        "  q = text_2_sent_vec(t.loc[\"Question\"])\n",
        "  sim_scores = []\n",
        "  for i in range(1,5):\n",
        "    sim = 1 - spatial.distance.cosine(q, text_2_sent_vec(t.loc[\"Answer\"+str(i)]))\n",
        "    sim_scores.append(sim)\n",
        "\n",
        "  # Use print to see all computed similarity scores\n",
        "  #print(k+1, sim_scores)\n",
        "  if t.loc[\"Type\"] == 'synonym':\n",
        "    prediction = t.loc[\"Answer\"+str(np.argmax(sim_scores) + 1)]\n",
        "  else:\n",
        "    prediction = t.loc[\"Answer\"+str(np.argmin(sim_scores) + 1)]\n",
        "\n",
        "  if prediction == t.loc[\"Correct\"]:\n",
        "    prediction_list.append(1)\n",
        "  else:\n",
        "    prediction_list.append(0)\n",
        "\n",
        "print(\"Correct answers: \" + str(np.count_nonzero(prediction_list)) +\"/\"+ str(len(prediction_list)) )"
      ],
      "metadata": {
        "id": "37E8U1sMU_v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet"
      ],
      "metadata": {
        "id": "cUndjJiTGKzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TY9i4cgIT7X",
        "outputId": "ecd67a2e-b0a4-4dd5-9a05-1737db32f23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "!unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jepPMh4RHxDa",
        "outputId": "6372f172-07d7-4282-8ea4-aebb1a05edf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /root/nltk_data/corpora/wordnet.zip\n",
            "   creating: /root/nltk_data/corpora/wordnet/\n",
            "  inflating: /root/nltk_data/corpora/wordnet/lexnames  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adv.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/cntlist.rev  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/LICENSE  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/citation.bib  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/noun.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/verb.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/README  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.sense  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adj.exc  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Recent NLTK versions require both packages for WordNet\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Sometimes necessary to manually unzip\n",
        "#!unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/\n",
        "\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCsCgqG1LoyI",
        "outputId": "28faeb64-8fc9-4cc3-9a84-9e8aada93a30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(nltk.find('corpora/wordnet.zip'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ9jbgjAKL6p",
        "outputId": "12660d4b-a230-4e3c-9d3d-24d15c773cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/nltk_data/corpora/wordnet.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hypernyms(text, method = 'first', second_text = ''):\n",
        "\n",
        "  # Retrieves the first discovered hypernym\n",
        "  # Ordering based on estimated frequency of usage\n",
        "  if method == 'first':\n",
        "    hypernyms = []\n",
        "    for syn in wn.synsets(text):\n",
        "      hypernyms = syn.hypernyms()\n",
        "      if len(hypernyms) > 0:\n",
        "        break\n",
        "\n",
        "    result = hypernyms[0].lemmas()[0].name()\n",
        "    return result\n",
        "\n",
        "  # Retrieves hypernym that is most relevant to 2nd text\n",
        "  elif method == 'relevant':\n",
        "\n",
        "    # Most common synset of second text\n",
        "    sec_syn = wn.synsets(second_text)[0]\n",
        "\n",
        "    first_synsets = wn.synsets(text)\n",
        "\n",
        "    best_score = 0\n",
        "    best_syn = None\n",
        "    for syn in first_synsets:\n",
        "      sim_score = syn.path_similarity(sec_syn)\n",
        "      if best_score < sim_score:\n",
        "        best_syn = syn\n",
        "\n",
        "    if best_syn is not None:\n",
        "      #print(best_syn)\n",
        "      hypernyms = best_syn.hypernyms()\n",
        "      result = hypernyms[0].lemmas()[0].name()\n",
        "      return result\n",
        "    else:\n",
        "      return\n",
        "\n",
        "  # Method undefined\n",
        "  else:\n",
        "    return"
      ],
      "metadata": {
        "id": "f_og4rYJGNZl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "print(get_hypernyms('diamond', method = 'first'))\n",
        "print(get_hypernyms('diamond', method = 'relevant', second_text = 'baseball'))\n",
        "\n",
        "print(get_hypernyms('park', method = 'first'))\n",
        "print(get_hypernyms('park', method = 'relevant', second_text = 'car'))\n",
        "\n",
        "print(get_hypernyms('kid', method = 'first'))\n",
        "print(get_hypernyms('kid', method = 'relevant', second_text = 'goat'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ5_6sB_HY4-",
        "outputId": "ff4f3519-e61e-4e3d-99da-67152e2e853d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jewel\n",
            "playing_field\n",
            "tract\n",
            "steer\n",
            "juvenile\n",
            "tease\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for syn in wn.synsets('kid'):\n",
        "  print(syn.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y78iChk9dqCl",
        "outputId": "08be35f1-602f-44ad-c434-6b206b65c651"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a young person of either sex\n",
            "soft smooth leather from the hide of a young goat\n",
            "English dramatist (1558-1594)\n",
            "a human offspring (son or daughter) of any age\n",
            "young goat\n",
            "tell false information to for fun\n",
            "be silly or tease one another\n"
          ]
        }
      ]
    }
  ]
}